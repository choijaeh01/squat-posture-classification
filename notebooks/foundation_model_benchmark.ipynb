{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 스쿼트 파운데이션 모델 벤치마크 노트북\n",
    "\n",
    "이 노트북은 라벨링된 스쿼트 데이터셋으로 CNN, CNN-GRU, ViT 기반 모델을 학습·평가해 성능을 비교하기 위한 베이스라인 워크플로를 제공합니다. 아래 순서를 따라가면서 데이터를 불러오고, 증강을 적용하고, 각 모델을 학습/검증/테스트할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 환경 설정 및 공통 유틸리티"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jae/Projects/squat_project/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import json\n",
    "import importlib\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Callable, Dict, Iterable, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# 프로젝트 소스 경로 자동 탐색\n",
    "NOTEBOOK_DIR = Path.cwd().resolve()\n",
    "CANDIDATES = [\n",
    "    NOTEBOOK_DIR,\n",
    "    NOTEBOOK_DIR.parent,\n",
    "    NOTEBOOK_DIR.parent.parent,\n",
    "]\n",
    "PROJECT_ROOT = None\n",
    "for candidate in CANDIDATES:\n",
    "    if (candidate / 'src').is_dir():\n",
    "        PROJECT_ROOT = candidate\n",
    "        break\n",
    "\n",
    "if PROJECT_ROOT is None:\n",
    "    raise RuntimeError('프로젝트 루트를 찾을 수 없습니다. 리포지토리 루트에서 노트북을 실행해 주세요.')\n",
    "\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from src.augmentations import (\n",
    "    add_gaussian_noise,\n",
    "    compose_transforms,\n",
    "    random_time_shift,\n",
    "    random_time_stretch,\n",
    "    random_time_warp,\n",
    "    random_scaling,\n",
    ")\n",
    "import src.data_loading as data_loading\n",
    "importlib.reload(data_loading)\n",
    "from src.data_loading import (\n",
    "    DatasetLayout,\n",
    "    SquatWindowDataset,\n",
    "    iter_class_counts,\n",
    "    make_dataloader,\n",
    "    train_val_test_split,\n",
    ")\n",
    "from src.models import TemporalCNNGRU\n",
    "\n",
    "torch.manual_seed(41)\n",
    "np.random.seed(41)\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "DEVICE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터셋 구성 및 탐색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetLayout(root=PosixPath('/Users/Jae/Projects/squat_project/data'))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터와 결과 저장 경로 설정\n",
    "DATA_ROOT = (PROJECT_ROOT / \"data\").resolve()\n",
    "RESULTS_DIR = (PROJECT_ROOT / \"results\" / \"foundation_models\").resolve()\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "layout = DatasetLayout(DATA_ROOT)\n",
    "layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 증강 파이프라인 정의 및 원본 데이터 로드\n",
    "TARGET_LENGTH = 512\n",
    "\n",
    "train_transforms = compose_transforms(\n",
    "    [\n",
    "        random_time_shift(max_shift=5),\n",
    "        random_scaling(0.9, 1.1),\n",
    "        random_time_stretch(0.85, 1.2),\n",
    "        random_time_warp(0.15),\n",
    "        add_gaussian_noise(0.01),\n",
    "    ]\n",
    ")\n",
    "\n",
    "raw_dataset = SquatWindowDataset(\n",
    "    layout.manually_labeled,\n",
    "    transforms=None,\n",
    "    drop_columns=('timestamp',),\n",
    "    target_length=TARGET_LENGTH,\n",
    ")\n",
    "len(raw_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CORRECT</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KNEE_VALGUS</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BUTT_WINK</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EXCESSIVE_LEAN</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PARTIAL_SQUAT</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            class  count\n",
       "0         CORRECT     80\n",
       "1     KNEE_VALGUS     80\n",
       "2       BUTT_WINK     80\n",
       "3  EXCESSIVE_LEAN     80\n",
       "4   PARTIAL_SQUAT     80"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 클래스별 샘플 분포 및 길이 확인\n",
    "counts = list(iter_class_counts(raw_dataset))\n",
    "lengths = {raw_dataset[idx][0].shape[1] for idx in range(len(raw_dataset))}\n",
    "print(f\"고유 윈도우 길이: {sorted(lengths)}\")\n",
    "pd.DataFrame([(cls.name, count) for cls, count in counts], columns=['class', 'count'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [23, 338] at entry 0 and [23, 265] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     23\u001b[39m val_loader = make_dataloader(val_dataset, batch_size=\u001b[32m64\u001b[39m, shuffle=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     24\u001b[39m test_loader = make_dataloader(test_dataset, batch_size=\u001b[32m64\u001b[39m, shuffle=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m].shape\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/squat_project/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:732\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    730\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    731\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    735\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    736\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    738\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/squat_project/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:788\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    787\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m788\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    789\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    790\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/squat_project/.venv/lib/python3.13/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/squat_project/.venv/lib/python3.13/site-packages/torch/utils/data/_utils/collate.py:398\u001b[39m, in \u001b[36mdefault_collate\u001b[39m\u001b[34m(batch)\u001b[39m\n\u001b[32m    337\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdefault_collate\u001b[39m(batch):\n\u001b[32m    338\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    339\u001b[39m \u001b[33;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[32m    340\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    396\u001b[39m \u001b[33;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[32m    397\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/squat_project/.venv/lib/python3.13/site-packages/torch/utils/data/_utils/collate.py:212\u001b[39m, in \u001b[36mcollate\u001b[39m\u001b[34m(batch, collate_fn_map)\u001b[39m\n\u001b[32m    208\u001b[39m transposed = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(*batch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m--> \u001b[39m\u001b[32m212\u001b[39m         \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    213\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[32m    214\u001b[39m     ]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/squat_project/.venv/lib/python3.13/site-packages/torch/utils/data/_utils/collate.py:155\u001b[39m, in \u001b[36mcollate\u001b[39m\u001b[34m(batch, collate_fn_map)\u001b[39m\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    154\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    157\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[32m    158\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/squat_project/.venv/lib/python3.13/site-packages/torch/utils/data/_utils/collate.py:272\u001b[39m, in \u001b[36mcollate_tensor_fn\u001b[39m\u001b[34m(batch, collate_fn_map)\u001b[39m\n\u001b[32m    270\u001b[39m     storage = elem._typed_storage()._new_shared(numel, device=elem.device)\n\u001b[32m    271\u001b[39m     out = elem.new(storage).resize_(\u001b[38;5;28mlen\u001b[39m(batch), *\u001b[38;5;28mlist\u001b[39m(elem.size()))\n\u001b[32m--> \u001b[39m\u001b[32m272\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: stack expects each tensor to be equal size, but got [23, 338] at entry 0 and [23, 265] at entry 1"
     ]
    }
   ],
   "source": [
    "# 학습/검증/테스트 분할 및 데이터로더 생성\n",
    "train_subset, val_subset, test_subset = train_val_test_split(raw_dataset, ratios=(0.7, 0.15, 0.15), seed=41)\n",
    "\n",
    "class AugmentedSubset(Dataset):\n",
    "    def __init__(self, subset: Subset, transform: Optional[Callable[[torch.Tensor], torch.Tensor]] = None) -> None:\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.subset)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        window, label = self.subset[index]\n",
    "        if self.transform is not None:\n",
    "            window = self.transform(window)\n",
    "        return window, label\n",
    "\n",
    "train_dataset = AugmentedSubset(train_subset, transform=train_transforms)\n",
    "val_dataset = AugmentedSubset(val_subset)\n",
    "test_dataset = AugmentedSubset(test_subset)\n",
    "\n",
    "train_loader = make_dataloader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = make_dataloader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = make_dataloader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "next(iter(train_loader))[0].shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 공통 학습 루틴 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    epochs: int = 30\n",
    "    learning_rate: float = 3e-4\n",
    "    weight_decay: float = 1e-3\n",
    "    grad_clip: float = 5.0\n",
    "\n",
    "\n",
    "def train_one_epoch(\n",
    "    model: nn.Module,\n",
    "    loader: DataLoader,\n",
    "    criterion: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    config: \"TrainingConfig\",\n",
    ") -> float:\n",
    "    model.train()\n",
    "    total_loss, total_samples = 0.0, 0\n",
    "\n",
    "    for batch, targets in tqdm(loader, desc=\"train\", leave=False):\n",
    "        batch, targets = batch.to(DEVICE), targets.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(batch)\n",
    "        loss = criterion(logits, targets)\n",
    "        loss.backward()\n",
    "        if config.grad_clip:\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), config.grad_clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * batch.size(0)\n",
    "        total_samples += batch.size(0)\n",
    "\n",
    "    return total_loss / max(total_samples, 1)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model: nn.Module, loader: DataLoader, criterion: nn.Module) -> Dict[str, float]:\n",
    "    model.eval()\n",
    "    total_loss, total_correct, total_samples = 0.0, 0, 0\n",
    "\n",
    "    for batch, targets in tqdm(loader, desc=\"eval\", leave=False):\n",
    "        batch, targets = batch.to(DEVICE), targets.to(DEVICE)\n",
    "        logits = model(batch)\n",
    "        loss = criterion(logits, targets)\n",
    "\n",
    "        total_loss += loss.item() * batch.size(0)\n",
    "        total_correct += (logits.argmax(dim=1) == targets).sum().item()\n",
    "        total_samples += batch.size(0)\n",
    "\n",
    "    return {\n",
    "        \"loss\": total_loss / max(total_samples, 1),\n",
    "        \"accuracy\": total_correct / max(total_samples, 1),\n",
    "    }\n",
    "\n",
    "\n",
    "def fit(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    config: \"TrainingConfig\",\n",
    ") -> Tuple[nn.Module, Dict[str, List[float]]]:\n",
    "    model = model.to(DEVICE)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    history = {\"train_loss\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "    best_score = 0.0\n",
    "    best_state = None\n",
    "\n",
    "    for epoch in range(1, config.epochs + 1):\n",
    "        train_loss = train_one_epoch(model, train_loader, criterion, optimizer, config)\n",
    "        metrics = evaluate(model, val_loader, criterion)\n",
    "\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"val_loss\"].append(metrics[\"loss\"])\n",
    "        history[\"val_acc\"].append(metrics[\"accuracy\"])\n",
    "\n",
    "        if metrics[\"accuracy\"] > best_score:\n",
    "            best_score = metrics[\"accuracy\"]\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch:03d} | train_loss={train_loss:.4f} | val_loss={metrics['loss']:.4f} | val_acc={metrics['accuracy']:.3%}\"\n",
    "        )\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    return model, history\n",
    "\n",
    "\n",
    "config = TrainingConfig(epochs=30, learning_rate=3e-4, weight_decay=1e-3, grad_clip=5.0)\n",
    "config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 모델 정의\n",
    "\n",
    "기본 CNN, CNN-GRU(TemporalCNNGRU), IMU 전용 ViT를 설정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNBaseline(nn.Module):\n",
    "    \"\"\"간단한 1D CNN 분류기.\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, num_classes: int) -> None:\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, 64, kernel_size=7, padding=3),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(64, 128, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class ViTSensor(nn.Module):\n",
    "    \"\"\"간단한 시계열 Vision Transformer.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        num_classes: int,\n",
    "        seq_len: int,\n",
    "        patch_size: int = 16,\n",
    "        dim: int = 128,\n",
    "        depth: int = 4,\n",
    "        heads: int = 4,\n",
    "        mlp_ratio: float = 2.0,\n",
    "        dropout: float = 0.1,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        assert seq_len % patch_size == 0, \"Sequence length must be divisible by patch size.\"\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = seq_len // patch_size\n",
    "        self.patch_dim = in_channels * patch_size\n",
    "\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            nn.Unfold(kernel_size=(1, patch_size), stride=(1, patch_size)),\n",
    "        )\n",
    "        self.linear_proj = nn.Linear(self.patch_dim, dim)\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, self.num_patches + 1, dim))\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=dim,\n",
    "            nhead=heads,\n",
    "            dim_feedforward=int(dim * mlp_ratio),\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=depth)\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        b, c, t = x.shape\n",
    "        x = x.unsqueeze(2)  # (b, c, 1, t)\n",
    "        patches = self.to_patch_embedding(x)  # (b, c * patch_size, num_patches)\n",
    "        patches = patches.transpose(1, 2)  # (b, num_patches, patch_dim)\n",
    "        tokens = self.linear_proj(patches)\n",
    "\n",
    "        cls_tokens = self.cls_token.repeat(b, 1, 1)\n",
    "        tokens = torch.cat([cls_tokens, tokens], dim=1)\n",
    "        tokens = tokens + self.pos_embedding[:, : tokens.size(1), :]\n",
    "\n",
    "        encoded = self.transformer(tokens)\n",
    "        return self.mlp_head(encoded[:, 0])\n",
    "\n",
    "class CNNLSTM(nn.Module):\n",
    "    \"\"\"CNN feature extractor followed by an LSTM head.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        num_classes: int,\n",
    "        conv_channels: int = 64,\n",
    "        hidden_size: int = 128,\n",
    "        num_layers: int = 1,\n",
    "        dropout: float = 0.1,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, conv_channels, kernel_size=7, padding=3),\n",
    "            nn.BatchNorm1d(conv_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(conv_channels, conv_channels, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm1d(conv_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=conv_channels,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=dropout if num_layers > 1 else 0.0,\n",
    "        )\n",
    "        self.head = nn.Sequential(\n",
    "            nn.LayerNorm(hidden_size * 2),\n",
    "            nn.Linear(hidden_size * 2, hidden_size),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        features = self.encoder(x)\n",
    "        sequence = features.transpose(1, 2)\n",
    "        output, _ = self.lstm(sequence)\n",
    "        pooled = output.mean(dim=1)\n",
    "        return self.head(pooled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 학습 및 검증\n",
    "\n",
    "모델별로 학습을 수행하고, 검증 성능을 비교합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_input_shape(loader: DataLoader) -> Tuple[int, int]:\n",
    "    sample, _ = next(iter(loader))\n",
    "    return sample.size(1), sample.size(2)\n",
    "\n",
    "\n",
    "in_channels, seq_len = infer_input_shape(train_loader)\n",
    "num_classes = 5\n",
    "\n",
    "model_factories: Dict[str, Callable[[], nn.Module]] = {\n",
    "    \"CNNBaseline\": lambda: CNNBaseline(in_channels, num_classes),\n",
    "    \"CNNLSTM\": lambda: CNNLSTM(in_channels, num_classes),\n",
    "    \"TemporalCNNGRU\": lambda: TemporalCNNGRU(in_channels=in_channels, num_classes=num_classes),\n",
    "    \"ViTSensor\": lambda: ViTSensor(in_channels=in_channels, num_classes=num_classes, seq_len=seq_len, patch_size=16),\n",
    "}\n",
    "\n",
    "histories: Dict[str, Dict[str, List[float]]] = {}\n",
    "val_metrics: Dict[str, Dict[str, float]] = {}\n",
    "trained_models: Dict[str, nn.Module] = {}\n",
    "\n",
    "for name, factory in model_factories.items():\n",
    "    print(f\"\\n=== Training: {name} ===\")\n",
    "    model = factory()\n",
    "    model, history = fit(model, train_loader, val_loader, config)\n",
    "    metrics = evaluate(model, val_loader, nn.CrossEntropyLoss())\n",
    "\n",
    "    histories[name] = history\n",
    "    val_metrics[name] = metrics\n",
    "    trained_models[name] = model\n",
    "\n",
    "pd.DataFrame(val_metrics).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 테스트 세트 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = {}\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for name, model in trained_models.items():\n",
    "    metrics = evaluate(model, test_loader, criterion)\n",
    "    test_results[name] = metrics\n",
    "    print(f\"{name} | test_loss={metrics['loss']:.4f} | test_acc={metrics['accuracy']:.3%}\")\n",
    "\n",
    "test_df = pd.DataFrame(test_results).T\n",
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 결과 저장 및 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = {\n",
    "    \"val_metrics\": val_metrics,\n",
    "    \"test_metrics\": test_results,\n",
    "    \"config\": config.__dict__,\n",
    "}\n",
    "\n",
    "summary_path = RESULTS_DIR / \"foundation_model_results.json\"\n",
    "with summary_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "summary_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "필요에 따라 `histories`를 활용해 학습 곡선을 시각화하거나, 혼동 행렬과 같은 추가 분석을 이어가세요."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}